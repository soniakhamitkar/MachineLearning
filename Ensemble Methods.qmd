---
title: "Assignment 8: Ensemble Methods"
author: "Sonia Khamitkar"
date: "11/15/2024"
output:
  rmdformats::readthedown:
    number_sections: true
    highlight: tango
    df_print: paged
    center: true
---

```{r setup, include=FALSE}
rm(list = ls()) # clears global environment
knitr::opts_knit$set(root.dir = '/Users/soniakhamitkar/Desktop/Babson Grad/Machine Learning/Data') 
```


# Context

Letâ€™s take a look at the data located in UniversalBank.csv. Each row represents a customer at small but rapidly growing bank. The columns measure all sorts of customer characteristics, ranging from their demographic information (e.g., Age, Family) to whether they currently have various accounts open with the bank (e.g., Securities Account, CD Account). For a complete description of the fields, consult the data page on Canvas.

The bank is aggressively trying to convert customers from depositors into borrowers through its personal loan program. The column **Personal Loan** shows whether each customer responded to a direct marketing campaign related to this program. The marketing team is now trying to understand what types of customers responds to new personal loans marketing. If they can establish reasonably strong predictive power, they will deploy the model more widely across their customer base to identify promising leads and a more nuanced target market.


## Question 1

Do the following:

* Import the data as "bank"

* Remove any predictor that might be inappropriate for this activity.

* Set the target to logical.

```{r}
bank = read.csv("UniversalBank.csv")
bank$ID <- NULL  # doesn't affect customer behavior
bank$ZIP.Code <- NULL  # doesn't affect customer behavior
bank$Personal.Loan = as.logical(bank$Personal.Loan) # target to logical
```


## Question 2

* Set a seed of 109 and partition a training set with 72% of the data.

* Run a Classification Tree to predict Personal.Loan, make the predictions, and calculate the error rate. What is the error rate?

Answer: The error rate is 1.64%

```{r}
# Set a seed of 109 and partition a training set with 72% of the data
set.seed(109)
N = nrow(bank)
trainingSize  = round(N*0.72)
trainingCases = sample(N, trainingSize)
training = bank[trainingCases,]
test = bank[-trainingCases,]

# Classification Tree
library(rpart)
model = rpart(Personal.Loan ~ ., data=training)
pred = predict(model, test)
pred = (pred > 0.5)
library(rpart.plot)
rpart.plot(model)

# Predictions, Error Rate
error_tree = sum(pred!=test$Personal.Loan)/nrow(test)
```


## Question 3

Bagging: Run a random forest model where you set the number of trees to 1000. Make the predictions and calculate the error rate. What is the error rate? Does this improve on the original single Classification Tree model?

Answer: The error rate is 1.28%. Yes it did improve the model as the the error rate decreased from 1.64% to 1.28% - thus the random forest model outperforms the classification tree model. Aggregating several trees to prevent over fitting and increase efficiency through the ensemble method of random forests has improved predicted correctness.

```{r}
#install.packages("randomForest")
library(randomForest)

set.seed(109) #because bagging is randomizing the data repeatedly, we have to make it replicable 
rf = randomForest(Personal.Loan ~ ., data=training, ntree=1000) # random forest function, ntree says how many trees we want to use  
pred_rf = predict(rf, test)
pred_rf = (pred_rf > 0.5)

error_randomForest = sum(pred_rf!=test$Personal.Loan)/nrow(test) # calculating errors 
```



## Question 4

* Boosting: Set a new seed to 23 and run a boosted tree model with 500 trees and 5 folds. Determine the best tree size. What is this size? Then use this "best tree size" to run another boosted tree model.

* Make the predictions and calculate the error rate. What is the error rate? If this error rate is worse than one or more of your previous models, what do you think might have happened?

Answer: The size is 500 for the first boosted tree model. Using the best tree size, the error rate is 2.07%. Since the boosting method's error rate of 2.07% is greater than the random forest's of 1.28%, the random forest model is not improved in this instance. Given that boosting frequently concentrates on incorrectly categorized cases, which may result in worse generalizations, this might be a consequence of over fitting in the boosting model.


```{r}
#install.packages("gbm") #gradient boosting machines aka gradient boosting trees 
library(gbm)
set.seed(23)
training$Personal.Loan=as.integer(training$Personal.Loan)
boost = gbm(Personal.Loan ~ ., data=training,n.trees=500, cv.folds=5) 
best_size <- gbm.perf(boost,method="cv")
best_size

# Run the model again with best tree size
boost = gbm(Personal.Loan ~ ., data=training,n.trees=500, cv.folds=5)
pred_boost  = predict(boost, test, n.trees=best_size, type="response")
pred_boost = (pred_boost > 0.5)
error_boost = sum(pred_boost != test$Personal.Loan)/nrow(test)

```


## Question 5

* Now, run a logistic regression model to predict Personal.Loan. Refine the model using the step() function.
* Make the predictions. Use a cutoff of 0.5. 
* Calculate the error rate. What is the error rate?

Answer: The error rate is 4.35%


```{r}
# logistic regression 
bank$Personal.Loan = as.logical(bank$Personal.Loan)
model1 <- glm(Personal.Loan ~ ., data=training, family=binomial)
summary(model1)

# refining
model2 <- step(model1)
summary(model2)

# predictions
test$predictions <- predict(model2, test, type="response")
predictions <- test$predictions
test$predictionsTF <- (test$predictions >= 0.5)
predictionsTF <- test$predictionsTF
observations <-test$Personal.Loan
table(predictionsTF, test$Personal.Loan)

# error rate
error_rate <- sum(predictionsTF != observations)/nrow(test)
error_rate
```


# Question 6

* Now, try to do stacking by combining the  Random Forest model, the Boosted model, and the logistic regression model. What is your error rate?

Answer: The error rate after stacking is 1.28%

```{r}
# stacking now 
pred_rf_full = predict(rf, bank)
pred_rf_full = (pred_rf_full > 0.5)

# boosting model
pred_boost_full = predict(boost, bank, n.trees=best_size, type="response")
pred_boost_full = (pred_boost_full > 0.5)

# refined model here
pred_model2_full = predict(model2, bank, type = "response")
pred_model2_full =  (pred_model2_full > 0.5)

# add refined model, personal loan to logical
bank_stacked = cbind(bank,pred_boost_full, pred_rf_full, pred_model2_full)
bank_stacked$Personal.Loan = as.logical(bank$Personal.Loan)
train_stacked = bank_stacked[trainingCases, ]
test_stacked = bank_stacked[-trainingCases, ]

# logistic regression formula
stacked = glm(Personal.Loan ~ ., data = train_stacked, family=binomial)

# get predictions
pred_stacked = predict(stacked, test_stacked, type="response")
pred_stacked = (pred_stacked > 0.5)

# error rate
error_stacked = sum(pred_stacked != test$Personal.Loan)/nrow(test)

```



## Question 7

Which ensemble method lowered the error the most? 

Answer: The random forest aka bagging lowered the error the most at 1.28% compared to 1.64% classification tree and 2.07% boosting

